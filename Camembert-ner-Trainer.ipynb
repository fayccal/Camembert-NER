{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T10:02:37.257515Z","iopub.status.busy":"2022-08-19T10:02:37.256951Z","iopub.status.idle":"2022-08-19T10:02:49.642924Z","shell.execute_reply":"2022-08-19T10:02:49.641290Z","shell.execute_reply.started":"2022-08-19T10:02:37.257452Z"},"trusted":true},"outputs":[],"source":["! pip install datasets seqeval"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T10:02:53.421449Z","iopub.status.busy":"2022-08-19T10:02:53.420261Z","iopub.status.idle":"2022-08-19T10:02:53.438298Z","shell.execute_reply":"2022-08-19T10:02:53.436656Z","shell.execute_reply.started":"2022-08-19T10:02:53.421412Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Ignore les pairs ou sont set les labels à -100 (inutile pour le calcule des perfs)\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = metric.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T10:05:22.049545Z","iopub.status.busy":"2022-08-19T10:05:22.047965Z","iopub.status.idle":"2022-08-19T10:06:45.696667Z","shell.execute_reply":"2022-08-19T10:06:45.695128Z","shell.execute_reply.started":"2022-08-19T10:05:22.049511Z"},"trusted":true},"outputs":[],"source":["from datasets import load_dataset, load_metric\n","from transformers import CamembertTokenizerFast, CamembertForTokenClassification\n","import transformers\n","from transformers import  TrainingArguments, Trainer\n","from transformers import DataCollatorForTokenClassification\n","\n","def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n","    labels = []\n","    for i, label in enumerate(examples[\"ner_tags\"]):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:\n","\n","            # Les tokens génerer par le tokenizer son set à -100\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            # Pour chaque premier token d'un mot on lui attribut le label\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(label[word_idx])\n","            # Les subtokens quant à eut sont set également à -100\n","            else:\n","                label_ids.append(-100)\n","            previous_word_idx = word_idx\n","        labels.append(label_ids)\n","\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs\n","\n","#load le dataset depuis huggingFace\n","datasets = load_dataset(\"Babelscape/wikineural\")\n","\n","#les différents labels\n","label_list = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n","labels_vocab = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n","labels_vocab_reverse = {v:k for k,v in labels_vocab.items()}\n","\n","#On utilisera seulement les datasets français\n","train_dataset = datasets[\"train_fr\"]\n","test_dataset = datasets[\"test_fr\"]\n","val_dataset = datasets[\"val_fr\"]\n","\n","first = datasets[\"train_fr\"][0]\n","\n","tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\")\n","\n","train_tokenized = train_dataset.map(tokenize_and_align_labels, batched=True)\n","test_tokenized = test_dataset.map(tokenize_and_align_labels, batched=True)\n","val_tokenized = val_dataset.map(tokenize_and_align_labels, batched=True)\n","\n","model = CamembertForTokenClassification.from_pretrained(\"camembert-base\", num_labels=len(label_list), label2id=labels_vocab, id2label=labels_vocab_reverse)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T10:07:54.367267Z","iopub.status.busy":"2022-08-19T10:07:54.366614Z","iopub.status.idle":"2022-08-19T10:08:03.820639Z","shell.execute_reply":"2022-08-19T10:08:03.819111Z","shell.execute_reply.started":"2022-08-19T10:07:54.367197Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","num_steps = len(train_dataset) // 16\n","model_name = \"camembert-base\"\n","args = TrainingArguments(\n","    \"Finetuned-camem-ner\",\n","    evaluation_strategy = \"steps\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=1,\n","    weight_decay=0.01,\n","    eval_steps=num_steps,\n","    save_steps=num_steps,\n",")\n","\n","#applique le padding sur no données tokeniser\n","data_collator = DataCollatorForTokenClassification(tokenizer)\n","#permet d'évaluer les performances du model, seqeval pouvant évaluer celle d'un modèle de NER\n","metric = load_metric(\"seqeval\")\n","#cette metric a besoin de la liste des labels\n","labels = [label_list[i] for i in first[\"ner_tags\"]]\n","\n","metric.compute(predictions=[labels], references=[labels])\n","\n","#definition du Trainer\n","trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=train_tokenized,\n","    eval_dataset=test_tokenized,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","#désactive wandb qui est utilisé par défaut par le trainer\n","os.environ[\"WANDB_DISABLED\"] = \"true\""]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T10:09:47.180847Z","iopub.status.busy":"2022-08-19T10:09:47.180362Z","iopub.status.idle":"2022-08-19T10:29:07.356819Z","shell.execute_reply":"2022-08-19T10:29:07.354424Z","shell.execute_reply.started":"2022-08-19T10:09:47.180816Z"},"trusted":true},"outputs":[],"source":["#phase de fine tuning et d'évaluation\n","trainer.train()\n","\n","trainer.evaluate()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T10:29:14.196888Z","iopub.status.busy":"2022-08-19T10:29:14.196272Z","iopub.status.idle":"2022-08-19T10:29:16.337593Z","shell.execute_reply":"2022-08-19T10:29:16.336243Z","shell.execute_reply.started":"2022-08-19T10:29:14.196837Z"},"trusted":true},"outputs":[],"source":["#sauvegarde du model\n","trainer.save_model('models/mycamembert-base-ner')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-12T15:00:18.601940Z","iopub.status.busy":"2022-08-12T15:00:18.600336Z","iopub.status.idle":"2022-08-12T15:00:49.523007Z","shell.execute_reply":"2022-08-12T15:00:49.521455Z","shell.execute_reply.started":"2022-08-12T15:00:18.601892Z"},"trusted":true},"outputs":[],"source":["#Zip du modèle pour être téléchargé si souhaiter\n","!zip -r model.zip models/mycamembert-base-ner"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T10:29:19.402811Z","iopub.status.busy":"2022-08-19T10:29:19.402404Z","iopub.status.idle":"2022-08-19T10:29:19.447508Z","shell.execute_reply":"2022-08-19T10:29:19.445930Z","shell.execute_reply.started":"2022-08-19T10:29:19.402778Z"},"trusted":true},"outputs":[],"source":["\n","import torch\n","\n","#test sur plusieurs paragraphe de texte notre reconnaissance d'entité nommée\n","\n","#sentence = \"Les Lumières sont un mouvement culturel, philosophique, littéraire et intellectuel qui émerge dans la seconde moitié du XVIIe siècle avec des philosophes comme Descartes, Spinoza, Locke, Bayle et Newton, avant de se développer dans toute l'Europe, notamment en France, au XVIIIe siècle. Par extension, on a donné à cette période le nom de siècle des Lumières. \"\n","sentence = \"Facebook est fondé en 2004 par Mark Zuckerberg et ses camarades de l'université Harvard : Chris Hughes, Eduardo Saverin, Andrew McCollum et Dustin Moskovitz. D'abord réservé aux étudiants de cette université, il s'est ensuite ouvert à d'autres universités américaines avant de devenir accessible à tous en septembre 2006.\"\n","#sentence = \"Charles André Joseph Marie de Gaulle naît le 22 novembre 1890 à 4 heures du matin, au 9 rue Princesse à Lille. Il est baptisé quelques heures après sa naissance en l'église Saint-André de Lille : son parrain est son oncle Gustave de Corbie et sa marraine sa tante Lucie Maillot née Droulers.\"\n","tokenized_sentence = tokenizer.encode(sentence)\n","input_ids = torch.tensor([tokenized_sentence])\n","\n","with torch.no_grad():\n","    output = model(input_ids.cuda())\n","    label_indices = np.argmax(output[0].to('cpu').numpy(),axis=2)\n","    \n","tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n","\n","new_tokens, new_labels = [], []\n","#une liste d'execptions nécessaire pour pouvoir bien relier les tokens\n","execeptions = ['<s>', '.', ',' ':', '?', '!']\n","for token, label_idx in zip(tokens, label_indices[0]):\n","    # Si le token commence par le caractère spécial ou est dans les execptions alors le token n'a pas besoin d'etre ajusté\n","    if token.startswith('▁') or token in execeptions:\n","        new_labels.append(label_list[label_idx])\n","        new_tokens.append(token)\n","    else:\n","        # Sinon on relie les tokens pour obtenir le mots entier\n","        new_tokens[-1] = new_tokens[-1] + token\n","\n","#Retirer le caractère spécial\n","for x in range(len(new_tokens)):\n","    new_tokens[x] = new_tokens[x].replace('▁', '')\n","    \n","names = []\n","curr_name = \"\"\n","\n","#On parcourt les tokens et récupere les tokens avec les labels correspondant à un noms\n","for i in range(len(new_tokens)):\n","    if new_labels[i] != 'O':\n","        leb = new_labels[i]\n","        if leb == \"B-PER\":\n","            if curr_name != \"\":\n","                curr_name += \" \"\n","            curr_name += new_tokens[i]\n","        \n","        if leb == \"I-PER\":\n","            if curr_name != \"\":\n","                curr_name += \" \"\n","            curr_name += new_tokens[i]\n","\n","    else:\n","        if curr_name != \"\":\n","            names.append(curr_name)\n","            curr_name = \"\"\n","\n","print(names)\n","cleaned_names = []\n","for v in range(len(names)):\n","   n = names[v].split(\", \")\n","   for m in n:\n","        cleaned_names.append(m)\n","print(cleaned_names)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T10:29:25.231875Z","iopub.status.busy":"2022-08-19T10:29:25.231456Z","iopub.status.idle":"2022-08-19T10:29:25.252147Z","shell.execute_reply":"2022-08-19T10:29:25.250107Z","shell.execute_reply.started":"2022-08-19T10:29:25.231843Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import tensorflow as tf\n","\n","class DistillationTrainingArguments(TrainingArguments):\n","    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        \n","        self.alpha = alpha\n","        self.temperature = temperature\n","\n","class DistillationTrainer(Trainer):\n","    def __init__(self, *args, teacher_model=None, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.teacher = teacher_model\n","        self._move_model_to_device(self.teacher,self.model.device)\n","        self.teacher.eval()\n","    #override de la méthode compute_loss\n","    def compute_loss(self, model, inputs, return_outputs = False):\n","        self._move_model_to_device(model,self.model.device)\n","        #forward pass du student model\n","        outputs_stu = model(**inputs)\n","        # Récupération de la loss\n","        st_loss = outputs_stu.loss\n","        \n","        with torch.no_grad():\n","            outputs_tea = self.teacher(**inputs)\n","            \n","        assert outputs_stu.logits.size() == outputs_tea.logits.size()\n","        # application de la kl divergence\n","        loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n","        loss_logits = (loss_function(\n","             F.log_softmax(outputs_stu.logits / self.args.temperature, dim=-1),\n","             F.softmax(outputs_tea.logits / self.args.temperature, dim=-1)) * (self.args.temperature ** 2))\n","\n","        loss_logits /= (outputs_stu.logits.shape[1] - 1)\n","        loss = self.args.alpha * loss_logits + (1. - self.args.alpha) * st_loss\n","        return (loss, outputs_stu) if return_outputs else loss"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T10:29:29.224011Z","iopub.status.busy":"2022-08-19T10:29:29.223497Z","iopub.status.idle":"2022-08-19T10:30:07.505065Z","shell.execute_reply":"2022-08-19T10:30:07.503281Z","shell.execute_reply.started":"2022-08-19T10:29:29.223977Z"},"trusted":true},"outputs":[],"source":["student_args = DistillationTrainingArguments(\n","    \"distil-ner\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=1,\n","    weight_decay=0.01,\n","    eval_steps=num_steps,\n","    save_steps=num_steps,\n","    temperature=2.0\n",")\n","\n","# chargement du teacher et student model\n","teacher_checkpoint = \"models/mycamembert-base-ner\"\n","student_checkpoint = \"cmarkea/distilcamembert-base\"\n","device = torch.device(\"cuda\")\n","teacher_model = CamembertForTokenClassification.from_pretrained(teacher_checkpoint, num_labels=len(label_list), label2id=labels_vocab, id2label=labels_vocab_reverse).to(device)\n","student_model = CamembertForTokenClassification.from_pretrained(student_checkpoint, num_labels=len(label_list), label2id=labels_vocab, id2label=labels_vocab_reverse).to(device)\n","\n","student_tokenizer = CamembertTokenizerFast.from_pretrained(student_checkpoint)\n","data_collator = DataCollatorForTokenClassification(student_tokenizer)\n","\n","# Définition du distil trainer\n","distil_trainer = DistillationTrainer(\n","    model=student_model,\n","    teacher_model=teacher_model,\n","    args=student_args,\n","    train_dataset=train_tokenized,\n","    eval_dataset=test_tokenized,\n","    data_collator=data_collator,\n","    tokenizer=student_tokenizer,\n","    compute_metrics=compute_metrics)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T10:30:07.516440Z","iopub.status.busy":"2022-08-19T10:30:07.512706Z","iopub.status.idle":"2022-08-19T10:44:54.406310Z","shell.execute_reply":"2022-08-19T10:44:54.404955Z","shell.execute_reply.started":"2022-08-19T10:30:07.516385Z"},"trusted":true},"outputs":[],"source":["distil_trainer.train()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T10:44:54.410526Z","iopub.status.busy":"2022-08-19T10:44:54.409047Z","iopub.status.idle":"2022-08-19T10:44:55.457145Z","shell.execute_reply":"2022-08-19T10:44:55.452558Z","shell.execute_reply.started":"2022-08-19T10:44:54.410475Z"},"trusted":true},"outputs":[],"source":["#sauvegarde du modèle distiller\n","distil_trainer.save_model('models/mydistil-camemebert-ner')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-12T15:18:41.615188Z","iopub.status.busy":"2022-08-12T15:18:41.614696Z","iopub.status.idle":"2022-08-12T15:19:01.945681Z","shell.execute_reply":"2022-08-12T15:19:01.944182Z","shell.execute_reply.started":"2022-08-12T15:18:41.615123Z"},"trusted":true},"outputs":[],"source":["#pour le téléchargement du modèle distiller\n","!zip -r modeldistil.zip models/mydistil-camemebert-ner"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T10:44:55.461104Z","iopub.status.busy":"2022-08-19T10:44:55.460055Z","iopub.status.idle":"2022-08-19T10:44:55.495182Z","shell.execute_reply":"2022-08-19T10:44:55.493735Z","shell.execute_reply.started":"2022-08-19T10:44:55.461037Z"},"trusted":true},"outputs":[],"source":["\n","import torch\n","\n","#test sur plusieurs paragraphe de texte notre reconnaissance d'entité nommée\n","\n","#sentence = \"Les Lumières sont un mouvement culturel, philosophique, littéraire et intellectuel qui émerge dans la seconde moitié du XVIIe siècle avec des philosophes comme Descartes, Spinoza, Locke, Bayle et Newton, avant de se développer dans toute l'Europe, notamment en France, au XVIIIe siècle. Par extension, on a donné à cette période le nom de siècle des Lumières. \"\n","sentence = \"Facebook est fondé en 2004 par Mark Zuckerberg et ses camarades de l'université Harvard : Chris Hughes, Eduardo Saverin, Andrew McCollum et Dustin Moskovitz. D'abord réservé aux étudiants de cette université, il s'est ensuite ouvert à d'autres universités américaines avant de devenir accessible à tous en septembre 2006.\"\n","#sentence = \"Charles André Joseph Marie de Gaulle naît le 22 novembre 1890 à 4 heures du matin, au 9 rue Princesse à Lille. Il est baptisé quelques heures après sa naissance en l'église Saint-André de Lille : son parrain est son oncle Gustave de Corbie et sa marraine sa tante Lucie Maillot née Droulers.\"\n","tokenized_sentence = tokenizer.encode(sentence)\n","input_ids = torch.tensor([tokenized_sentence])\n","\n","with torch.no_grad():\n","    output = student_model(input_ids.to(device))\n","    label_indices = np.argmax(output[0].to('cpu').numpy(),axis=2)\n","    \n","tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n","\n","new_tokens, new_labels = [], []\n","#une liste d'execptions nécessaire pour pouvoir bien relier les tokens\n","execeptions = ['<s>', '.', ',' ':', '?', '!']\n","for token, label_idx in zip(tokens, label_indices[0]):\n","    # Si le token commence par le caractère spécial ou est dans les execptions alors le token n'a pas besoin d'etre ajusté\n","    if token.startswith('▁') or token in execeptions:\n","        new_labels.append(label_list[label_idx])\n","        new_tokens.append(token)\n","    else:\n","        # Sinon on relie les tokens pour obtenir le mots entier\n","        new_tokens[-1] = new_tokens[-1] + token\n","\n","#Retirer le caractère spécial\n","for x in range(len(new_tokens)):\n","    new_tokens[x] = new_tokens[x].replace('▁', '')\n","    \n","names = []\n","curr_name = \"\"\n","\n","#On parcourt les tokens et récupere les tokens avec les labels correspondant à un noms\n","for i in range(len(new_tokens)):\n","    if new_labels[i] != 'O':\n","        leb = new_labels[i]\n","        if leb == \"B-PER\":\n","            if curr_name != \"\":\n","                curr_name += \" \"\n","            curr_name += new_tokens[i]\n","        \n","        if leb == \"I-PER\":\n","            if curr_name != \"\":\n","                curr_name += \" \"\n","            curr_name += new_tokens[i]\n","\n","    else:\n","        if curr_name != \"\":\n","            names.append(curr_name)\n","            curr_name = \"\"\n","\n","print(names)\n","cleaned_names = []\n","for v in range(len(names)):\n","   n = names[v].split(\", \")\n","   for m in n:\n","        cleaned_names.append(m)\n","print(cleaned_names)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T10:47:47.040830Z","iopub.status.busy":"2022-08-19T10:47:47.040347Z","iopub.status.idle":"2022-08-19T10:47:47.050075Z","shell.execute_reply":"2022-08-19T10:47:47.048289Z","shell.execute_reply.started":"2022-08-19T10:47:47.040796Z"},"trusted":true},"outputs":[],"source":["from transformers import pipeline\n","#import des pipeline aidant à la classification à partir d'un modèle"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-08-19T10:59:26.614349Z","iopub.status.busy":"2022-08-19T10:59:26.613760Z","iopub.status.idle":"2022-08-19T11:00:38.033710Z","shell.execute_reply":"2022-08-19T11:00:38.032257Z","shell.execute_reply.started":"2022-08-19T10:59:26.614293Z"},"trusted":true},"outputs":[],"source":["# Test la classification du modèle sur le dataset de validation\n","predictions, labels, _ = distil_trainer.predict(val_tokenized)\n","predictions = np.argmax(predictions, axis=2)\n","\n","true_predictions = [\n","    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","    for prediction, label in zip(predictions, labels)\n","]\n","true_labels = [\n","    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","    for prediction, label in zip(predictions, labels)\n","]\n","\n","results = metric.compute(predictions=true_predictions, references=true_labels)\n","results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-08-12T15:01:30.222580Z","iopub.status.idle":"2022-08-12T15:01:30.223899Z","shell.execute_reply":"2022-08-12T15:01:30.223585Z","shell.execute_reply.started":"2022-08-12T15:01:30.223492Z"},"trusted":true},"outputs":[],"source":["predictions, labels, _ = trainer.predict(val_tokenized)\n","predictions = np.argmax(predictions, axis=2)\n","\n","true_predictions = [\n","    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","    for prediction, label in zip(predictions, labels)\n","]\n","true_labels = [\n","    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","    for prediction, label in zip(predictions, labels)\n","]\n","\n","results = metric.compute(predictions=true_predictions, references=true_labels)\n","results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","#test de la vitesse des 2 modèles pour 1000 phrases de texte.\n","teacher_model = teacher_model.to('cpu')\n","for idx in range(1000):\n","    nlp = pipeline(\"ner\", model=teacher_model, tokenizer=tokenizer)\n","    joined = ' '.join(val_dataset[idx]['tokens'])\n","    ner_results = nlp(joined)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","#test de la vitesse des 2 modèles pour 1000 phrases de texte.)\n","student_model = student_model.to(\"cpu\")\n","for idx in range(1000):\n","    nlp = pipeline(\"ner\", model=student_model, tokenizer=student_tokenizer)\n","    joined = ' '.join(val_dataset[idx]['tokens'])\n","    ner_results = nlp(joined)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
